{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Sunan Kalijaga Yogyakarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                      university  \\\n",
      "0  67408  2024  UIN SUNAN KALIJAGA YOGYAKARTA.   \n",
      "1  65638  2024  UIN SUNAN KALIJAGA YOGYAKARTA.   \n",
      "2  64968  2024  UIN SUNAN KALIJAGA YOGYAKARTA.   \n",
      "3  65640  2024  UIN SUNAN KALIJAGA YOGYAKARTA.   \n",
      "4  65481  2024  UIN SUNAN KALIJAGA YOGYAKARTA.   \n",
      "\n",
      "                                               title  \\\n",
      "0  OPTIMISASI ANALISIS REGRESI LASSO DENGAN ALGOR...   \n",
      "1  PEMBENTUKAN PORTOFOLIO ROBUST MEAN-VARIANCE ME...   \n",
      "2  PERBANDINGAN MODEL MIXED GEOGRAPHICALLY WEIGHT...   \n",
      "3  PEMODELAN INDEKS GINI RATIO INDONESIA DENGAN P...   \n",
      "4  PERAMALAN INDEKS HARGA SAHAM MENGGUNAKAN METOD...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Linear regression is one of the modeling metho...  \n",
      "1  Portfolio is a collection of several assets. T...  \n",
      "2  The employment size in Indonesia, particularly...  \n",
      "3  Dynamic panel data regression model is a model...  \n",
      "4  Artificial Neural Network (ANN) adalah model l...  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://digilib.uin-suka.ac.id/view/divisions/jur=5Fmat/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://digilib.uin-suka.ac.id/\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"https://digilib.uin-suka.ac.id/id/eprint/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p', style=\"text-align: justify; margin: 1em auto 0em auto\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"UIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('em')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinsuka = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinsuka.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinsuka2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinsuka2.csv'\n",
    "\n",
    "# Pastikan folder tempat menyimpan file ada\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Cek apakah file sudah ada\n",
    "    if os.path.exists(filename):\n",
    "        # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "        existing_data = pd.read_csv(filename, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "        updated_data = pd.concat([existing_data, df_mtkuinsuka], ignore_index=True)\n",
    "    else:\n",
    "        # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "        updated_data = df_mtkuinsuka\n",
    "\n",
    "    # Simpan DataFrame gabungan ke file CSV\n",
    "    updated_data.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Sunan Gunung Djati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                         university  \\\n",
      "0  94061  2024    UIN Sunan Gunung Djati Bandung.   \n",
      "1  95299  2024      UIN Sunan Gung Djati Bandung.   \n",
      "2  93678  2024  : UIN Sunan Gunung Djati Bandung.   \n",
      "3  90307  2024    UIN Sunan Gunung Djati Bandung.   \n",
      "4  94198  2024    UIN Sunan Gunung Djati Bandung.   \n",
      "\n",
      "                                               title  \\\n",
      "0  Penyelesaian masalah transportasi multi-objekt...   \n",
      "1  Algoritma el-gamal pada teknik kriptografi aff...   \n",
      "2  Identifikasi faktor-faktor yang memengaruhi pe...   \n",
      "3  Bifurkasi Backward pada model HIV dalam tubuh ...   \n",
      "4  Nilai total ketakteraturan sisi Modular pada G...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Pembangunan berkelanjutan diperlakukan sebagai...  \n",
      "1  Skripsi ini membahas kriptografi kunci publik ...  \n",
      "2  Analisis survival adalah prosedur statistika u...  \n",
      "3  Human Immunodeficiency Virus (HIV) telah menja...  \n",
      "4                                               None  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://digilib.uinsgd.ac.id/view/divisions/prodi=5Fmatematika/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://digilib.uinsgd.ac.id/\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"https://digilib.uinsgd.ac.id/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p', style=\"text-align: justify; margin: 1em auto 0em auto; line-height:1.5em; white-space:pre-line\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"UIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('em')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinsgd = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinsgd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinsgd2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinsgd2.csv'\n",
    "\n",
    "# Cek apakah file sudah ada\n",
    "if os.path.exists(filename):\n",
    "    # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "    existing_data = pd.read_csv(filename)\n",
    "    \n",
    "    # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "    updated_data = pd.concat([existing_data, df_mtkuinsgd], ignore_index=True)\n",
    "else:\n",
    "    # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "    updated_data = df_mtkuinsgd\n",
    "\n",
    "# Simpan DataFrame gabungan ke file CSV\n",
    "updated_data.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Walisongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page for ID 10388. Status code: 404\n",
      "Failed to fetch page for ID 12293. Status code: 404\n",
      "Failed to fetch page for ID 12292. Status code: 404\n",
      "Failed to fetch page for ID 10387. Status code: 404\n",
      "      id  year university title abstract\n",
      "0  10388  None       None  None     None\n",
      "1  12293  None       None  None     None\n",
      "2  12292  None       None  None     None\n",
      "3  10387  None       None  None     None\n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://eprints.walisongo.ac.id/view/divisions/jur=5Fmat/2019.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://eprints.walisongo.ac.id\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"https://digilib.uinsgd.ac.id/id/eprint/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p.ep_field_para', style=\"text-align: left; margin: 1em auto 0em auto\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('p', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"UIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinwls = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinwls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Siber Syekh Nurjati Cirebon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  year                                        university  \\\n",
      "0  5624  None                      © IAIN Syekh Nurjati Cirebon   \n",
      "1  8500  2022  S1 Tadris Matematika IAIN Syekh Nurjati Cirebon.   \n",
      "2  8905  2021  S1 Tadris Matematika IAIN Syekh Nurjati Cirebon.   \n",
      "3    32  None                      © IAIN Syekh Nurjati Cirebon   \n",
      "4  6293  None                      © IAIN Syekh Nurjati Cirebon   \n",
      "\n",
      "                                               title  \\\n",
      "0  Penggunaan Bahan Ajar Berbasis Multimedia Inte...   \n",
      "1  PENGARUH PENGGUNAAN APLIKASI WORDWALL TERHADAP...   \n",
      "2  ANALISIS KETERAMPILAN GURU MATEMATIKA SE-KABUP...   \n",
      "3  Migration routes of the Common Shoveler  - IAI...   \n",
      "4  Analysis of mathematics test items quality for...   \n",
      "\n",
      "                                            abstract  \n",
      "0                                               None  \n",
      "1  Peningkatan kualitas proses dan hasil pembelaj...  \n",
      "2  Permasalahan yang dihadapi pada masa pandemic ...  \n",
      "3  This is where the abstract of this record woul...  \n",
      "4                                               None  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://repository.syekhnurjati.ac.id/view/subjects/QA.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://repository.syekhnurjati.ac.id\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"https://repository.syekhnurjati.ac.id/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p', style=\"text-align: left; margin: 1em auto 0em auto\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"IAIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinssn = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinssn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinssn.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinssn.csv'\n",
    "\n",
    "# Cek apakah file sudah ada\n",
    "if os.path.exists(filename):\n",
    "    # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "    existing_data = pd.read_csv(filename)\n",
    "    \n",
    "    # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "    updated_data = pd.concat([existing_data, df_mtkuinssn], ignore_index=True)\n",
    "else:\n",
    "    # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "    updated_data = df_mtkuinssn\n",
    "\n",
    "# Simpan DataFrame gabungan ke file CSV\n",
    "updated_data.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Sunan Ampel Surabaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                 university  \\\n",
      "0  67895  2024  UIN Sunan Ampel Surabaya.   \n",
      "1  72204  2024  UIN Sunan Ampel Surabaya.   \n",
      "2  67995  2024  UIN Sunan Ampel Surabaya.   \n",
      "3  71334  2024  UIN Sunan Ampel Surabaya.   \n",
      "4  72196  2024  UIN Sunan Ampel Surabaya.   \n",
      "\n",
      "                                               title  \\\n",
      "0  Deteksi down syndrome pada anak berdasarkan ci...   \n",
      "1  Optimalisasi metode K-Means clustering dengan ...   \n",
      "2  Pengukuran kinerja organisasi pada BPS Kabupat...   \n",
      "3  Penerapan Metode Autoregressive Distributed La...   \n",
      "4  Analisis pengendalian kualitas produksi gula m...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Down syndrome merupakan penyakit genetik yang ...  \n",
      "1  Klub sepak bola pasti memiliki keinginan yang ...  \n",
      "2  Pengukuran kinerja adalah suatu proses untuk m...  \n",
      "3  Produk Domestik Regional Bruto (PDRB) mencermi...  \n",
      "4  Pengendalian kualitas merupakan aspek krusial ...  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://digilib.uinsa.ac.id/view/divisions/fst=5Fmat/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://digilib.uinsa.ac.id/\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"https://digilib.uinsa.ac.id/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p', class_=\"ep_field_para\", align=\"justify\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"UIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinsa = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinsa.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinsa2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinsa2.csv'\n",
    "\n",
    "# Cek apakah file sudah ada\n",
    "if os.path.exists(filename):\n",
    "    # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "    existing_data = pd.read_csv(filename)\n",
    "    \n",
    "    # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "    updated_data = pd.concat([existing_data, df_mtkuinsa], ignore_index=True)\n",
    "else:\n",
    "    # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "    updated_data = df_mtkuinsa\n",
    "\n",
    "# Simpan DataFrame gabungan ke file CSV\n",
    "updated_data.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Syarif Hidayatullah Jakarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                                         university  \\\n",
      "0  76184  2024  Fakultas Sains dan Teknologi UIN Syarif HIdaya...   \n",
      "1  76238  2024                                               None   \n",
      "2  76263  2024  Fakultas Sains dan Teknologi UIN Syarif HIdaya...   \n",
      "3  76264  2024  Fakultas Sains dan Teknologi UIN Syarif Hidaya...   \n",
      "4  76351  2024  Fakultas Sains dan Teknologi UIN Syarif HIdaya...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Perhitungan premi asuransi jiwa berjangka tanp...   \n",
      "1  Pengaruh Early Warning System, Return On Asset...   \n",
      "2  Solusi model fluida persamaan pelat termoelast...   \n",
      "3  Operator solusi sistem persamaan Resolvent flu...   \n",
      "4  Penerapan model singular spectrum analysis dal...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Premi adalah sejumlah uang yang harus dibayark...  \n",
      "1  Untuk menumbuhkan kepercayaan nasabah pada ind...  \n",
      "2  Pelat termoelastik adalah struktur datar tipis...  \n",
      "3  Pada penelitian ini, dibahas proses penyelesai...  \n",
      "4  Di Indonesia, krisis ekonomi dapat dilihat dar...  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman utama yang ingin di-scrape\n",
    "base_url = \"https://repository.uinjkt.ac.id/dspace/handle/123456789/59/simple-search?query=&filter_field_1=dateIssued&filter_type_1=equals&filter_value_1=%5B2019+TO+2025%5D&sort_by=dc.date.issued_dt&order=asc&rpp=100&etal=0&start=200\"\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Cari semua elemen <a> dengan href mengarah ke repository UIN\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"/dspace/handle/123456789/\"):\n",
    "                id_value = href.split('/')[-1]  # Ambil ID dari URL\n",
    "                ids.append(id_value)\n",
    "\n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    url = f\"https://repository.uinjkt.ac.id/dspace/handle/123456789/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        rows = soup.find_all('tr')  # Ambil semua baris tabel\n",
    "\n",
    "        # Default values jika tidak ditemukan\n",
    "        year = None\n",
    "        title = None\n",
    "        university = None\n",
    "        abstract = None\n",
    "\n",
    "        # Loop setiap baris tabel untuk mencari metadata yang sesuai\n",
    "        for row in rows:\n",
    "            label = row.find('td', class_='metadataFieldLabel')\n",
    "            value = row.find('td', class_='metadataFieldValue')\n",
    "\n",
    "            if label and value:\n",
    "                text_label = label.text.strip()\n",
    "                text_value = value.text.strip()\n",
    "\n",
    "                if \"Issue Date\" in text_label:\n",
    "                    match = re.search(r'\\d{4}', text_value)\n",
    "                    if match:\n",
    "                        year = match.group(0)  # Ambil tahun (4 digit)\n",
    "                \n",
    "                if \"Title\" in text_label:\n",
    "                    title = text_value\n",
    "                \n",
    "                if \"UIN\" in text_value:  # Deteksi universitas\n",
    "                    university = text_value\n",
    "\n",
    "                if \"Abstract\" in text_label:\n",
    "                    abstract = text_value\n",
    "\n",
    "        return {\n",
    "            \"id\": id_value,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_value}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_value, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka (maksimum 100)\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[:100]  # Ambil maksimal 100 ID\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinshj = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinshj.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinshj2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinshj2.csv'\n",
    "\n",
    "# Cek apakah file sudah ada\n",
    "if os.path.exists(filename):\n",
    "    # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "    existing_data = pd.read_csv(filename)\n",
    "    \n",
    "    # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "    updated_data = pd.concat([existing_data, df_mtkuinshj], ignore_index=True)\n",
    "else:\n",
    "    # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "    updated_data = df_mtkuinshj\n",
    "\n",
    "# Simpan DataFrame gabungan ke file CSV\n",
    "updated_data.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Sumatera Utara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                                         university  \\\n",
      "0  22810  2024  © 2022 PUSTIPADA UIN Sumatera Utara - Powered ...   \n",
      "1  23131  2024                                UIN Surmatra Utara.   \n",
      "2  23896  2024                          UIN Sumatera Utara Medan.   \n",
      "3  24229  2024                          UIN Sumatera Utara Medan.   \n",
      "4  24457  2024                                UIN Sumatera Utara.   \n",
      "\n",
      "                                               title  \\\n",
      "0  PENGENDALIAN KUALITAS AIR SUNGAI DELI KOTA\\r\\n...   \n",
      "1  Penerapan Metode Multi Attribute Utility Theor...   \n",
      "2  Penerapan Partial Least Square-Modified Fuzzy\\...   \n",
      "3  Penerapan Algoritma Tabu Search Pada Capacitat...   \n",
      "4  Penerapan Metode Chi-Square Automatic\\r\\nInter...   \n",
      "\n",
      "                                            abstract  \n",
      "0  Clean water is a type of water-based resource ...  \n",
      "1  Beasiswa UPZ merupakan beasiswa yang bersumber...  \n",
      "2  JNE Express merupakan salah satu perusahaan ya...  \n",
      "3  Kegiatan pengangkutan sampah merupakan salah s...  \n",
      "4  Tujuan penelitian ini adalah untuk mengklasifi...  \n"
     ]
    }
   ],
   "source": [
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"http://repository.uinsu.ac.id/view/divisions/SkripsiMAT/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# try:\n",
    "#     response = requests.get(base_url, timeout=30)  # Timeout diperpanjang menjadi 30 detik\n",
    "#     print(response.text)\n",
    "# except requests.exceptions.Timeout:\n",
    "#     print(\"Request timed out. Coba lagi nanti.\")\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke https://\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"http://repository.uinsu.ac.id/\"):\n",
    "                # Ambil ID dari URL (angka setelah domain)\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID sebelum tanda \"/\"\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    # Bentuk URL menggunakan pola yang benar\n",
    "    url = f\"http://repository.uinsu.ac.id/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Tambahkan proses scraping di sini sesuai kebutuhan\n",
    "        \n",
    "        # Ambil paragraf abstrak\n",
    "        paragraph = soup.find('p', style=\"text-align: left; margin: 1em auto 0em auto\")  # Perbaiki selector\n",
    "        abstract = paragraph.text.strip() if paragraph else None\n",
    "        if abstract:\n",
    "            abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"UIN\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('em')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_link,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_link}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_link, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 9 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[0:100]  # Indeks dimulai dari 0\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap link ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinsumut = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinsumut.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinsumut2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinsumut2.csv'\n",
    "\n",
    "# Pastikan folder tempat menyimpan file ada\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Cek apakah file sudah ada\n",
    "    if os.path.exists(filename):\n",
    "        # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "        existing_data = pd.read_csv(filename, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "        updated_data = pd.concat([existing_data, df_mtkuinsumut], ignore_index=True)\n",
    "    else:\n",
    "        # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "        updated_data = df_mtkuinsumut\n",
    "\n",
    "    # Simpan DataFrame gabungan ke file CSV\n",
    "    updated_data.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UIN Maulana Malik Ibrahim Malang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  year                                       university  \\\n",
      "0  71316  2024  Universitas Islam Negeri Maulana Malik Ibrahim.   \n",
      "1  65301  2024  Universitas Islam Negeri Maulana Malik Ibrahim.   \n",
      "2  71223  2024  Universitas Islam Negeri Maulana Malik Ibrahim.   \n",
      "3  71351  2024  Universitas Islam Negeri Maulana Malik Ibrahim.   \n",
      "4  71349  2024  Universitas Islam Negeri Maulana Malik Ibrahim.   \n",
      "\n",
      "                                               title  \\\n",
      "0  Impelementasi metode mean shift untuk clusteri...   \n",
      "1  Identifikasi autokorelasi spasial sebaran peti...   \n",
      "2  Implementasi algoritma support vector machine ...   \n",
      "3  Randic, Sum-Connectivity Indeks dan koindeks p...   \n",
      "4  Randic, sum-connectivity indeks dan koindeks p...   \n",
      "\n",
      "                                            abstract  \n",
      "0  INDONESIA : Identifikasi daerah rawan kejadian...  \n",
      "1  ABSTRAK Data spasial merupakan data dengan asp...  \n",
      "2  INDONESIA: Diabetes Mellitus adalah penyakit k...  \n",
      "3  INDONESIA: Penelitian ini mengkaji tentang Ran...  \n",
      "4  INDONESIA: Penelitian ini mengkaji tentang Ran...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"http://etheses.uin-malang.ac.id/view/divisions/JMat/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke halaman detail\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"http://etheses.uin-malang.ac.id/\"):\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID dari URL\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    url = f\"http://etheses.uin-malang.ac.id/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Ambil paragraf abstrak menggunakan XPath\n",
    "        tree = html.fromstring(response.content)\n",
    "        xpath_expression = '//p[@class=\"ep_field_para\" and @align=\"justify\"]/text()'\n",
    "        abstract_list = tree.xpath(xpath_expression)\n",
    "        abstract = \" \".join(abstract_list).strip() if abstract_list else None\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"Universitas\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('em')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_value,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_value}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_value, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 0 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[:100]  # Ambil maksimum 100 ID\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinmalik = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinmalik.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_uinmalik2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_uinmalik2.csv'\n",
    "\n",
    "# Pastikan folder tempat menyimpan file ada\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Cek apakah file sudah ada\n",
    "    if os.path.exists(filename):\n",
    "        # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "        existing_data = pd.read_csv(filename, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "        updated_data = pd.concat([existing_data, df_mtkuinmalik], ignore_index=True)\n",
    "    else:\n",
    "        # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "        updated_data = df_mtkuinmalik\n",
    "\n",
    "    # Simpan DataFrame gabungan ke file CSV\n",
    "    updated_data.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping UIN Walisongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page. Status code: 404\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "# URL dari halaman yang ingin di-scrape\n",
    "base_url = \"https://eprints.walisongo.ac.id/view/divisions/jur=5Fmat/2024.html\"  # Ganti dengan URL target\n",
    "\n",
    "# Fungsi untuk melakukan scraping ID dari link\n",
    "def scrape_ids(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Cari semua elemen <a> dengan href mengarah ke halaman detail\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        # Filter hanya link yang sesuai pola dan ekstrak ID\n",
    "        ids = []\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://eprints.walisongo.ac.id/\"):\n",
    "                id_value = href.split('/')[-2]  # Mengambil ID dari URL\n",
    "                ids.append(id_value)\n",
    "        \n",
    "        return ids\n",
    "    else:\n",
    "        print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Fungsi untuk mengambil data detail dari halaman berdasarkan ID\n",
    "def scrape_detail(id_value):\n",
    "    url = f\"https://eprints.walisongo.ac.id/id/eprint/{id_value}/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Ambil paragraf abstrak menggunakan XPath\n",
    "        tree = html.fromstring(response.content)\n",
    "        xpath_expression = '//*[@id=\"eprints_content\"]/div/div[4]/div[3]'\n",
    "        abstract_list = tree.xpath(xpath_expression)\n",
    "        abstract = \" \".join([elem.text_content().strip() for elem in abstract_list]) if abstract_list else None\n",
    "\n",
    "        # Ambil paragraf abstrak\n",
    "        # paragraph = soup.find('p', style=\"text-align: left; margin: 1em auto 0em auto\")  # Perbaiki selector\n",
    "        # abstract = paragraph.text.strip() if paragraph else None\n",
    "        # if abstract:\n",
    "        #     abstract = f\"{abstract}\"\n",
    "        \n",
    "        # Ambil tahun\n",
    "        year_tag = soup.find('span', class_='person_name')\n",
    "        if year_tag and year_tag.next_sibling:\n",
    "            sibling_text = year_tag.next_sibling.strip()\n",
    "            match = re.search(r'\\((\\d{4})\\)', sibling_text)\n",
    "            year = match.group(1) if match else None\n",
    "        else:\n",
    "            year = None\n",
    "\n",
    "        # Ambil universitas\n",
    "        paragraphs = soup.find_all('p')\n",
    "        university = None\n",
    "        for para in paragraphs:\n",
    "            if \"Universitas\" in para.text:\n",
    "                university = para.text.split(\",\")[-1].strip()\n",
    "                break\n",
    "                    \n",
    "        # Ambil judul\n",
    "        title_tag = soup.find('em')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        return {\n",
    "            \"id\": id_value,\n",
    "            \"year\": year,\n",
    "            \"university\": university,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch page for ID {id_value}. Status code: {response.status_code}\")\n",
    "        return {\"id\": id_value, \"year\": None, \"university\": None, \"title\": None, \"abstract\": None}\n",
    "\n",
    "# Langkah 1: Scrape semua ID dari halaman utama\n",
    "ids = scrape_ids(base_url)\n",
    "\n",
    "# Langkah 2: Filter hanya ID berupa angka, mulai dari nomor 0 hingga maksimum 100\n",
    "filtered_ids = [item for item in ids if item.isdigit()]\n",
    "ids_from_0_to_100 = filtered_ids[:100]  # Ambil maksimum 100 ID\n",
    "\n",
    "# Langkah 3: Ambil detail dari setiap ID\n",
    "result = []\n",
    "for id_link in ids_from_0_to_100:\n",
    "    details = scrape_detail(id_link)\n",
    "    result.append(details)\n",
    "\n",
    "# Langkah 4: Masukkan data ke dalam DataFrame\n",
    "df_mtkuinwalisongo = pd.DataFrame(result)\n",
    "\n",
    "# Langkah 5: Tampilkan DataFrame\n",
    "print(df_mtkuinwalisongo.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame telah diperbarui dan disimpan ke file './csv/mtk_walisongo2.csv'\n"
     ]
    }
   ],
   "source": [
    "# Nama file CSV\n",
    "filename = './csv/mtk_walisongo2.csv'\n",
    "\n",
    "# Pastikan folder tempat menyimpan file ada\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Cek apakah file sudah ada\n",
    "    if os.path.exists(filename):\n",
    "        # Jika file sudah ada, baca file CSV ke dalam DataFrame\n",
    "        existing_data = pd.read_csv(filename, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # Tambahkan data baru ke DataFrame yang sudah ada\n",
    "        updated_data = pd.concat([existing_data, df_mtkuinwalisongo], ignore_index=True)\n",
    "    else:\n",
    "        # Jika file belum ada, data baru menjadi DataFrame awal\n",
    "        updated_data = df_mtkuinwalisongo\n",
    "\n",
    "    # Simpan DataFrame gabungan ke file CSV\n",
    "    updated_data.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"DataFrame telah diperbarui dan disimpan ke file '{filename}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
